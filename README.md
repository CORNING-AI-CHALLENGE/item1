# Corning AI challenge _ ITEM 1

### ğŸ´â€â˜ ï¸ Large language models (LLMs)ë¥¼ ì´ìš©í•œ query engine

<br/>

ğŸ’¡ Purpose of project:

With the vast amount of data available on the internet, it has become increasingly challenging for users to find relevant information quickly and efficiently. Traditional search engines rely on keywords and algorithms to rank search results, which can be limiting and often fail to provide accurate and relevant information. To address this issue, weâ€™d like to develop a query engine that leverages the power of large language models to provide more accurate and efficient search results.

<br/>

ğŸ”‘ Objectives:

The primary objective of this project is to develop a query engine that utilizes large language models to understand the intent behind a user's search query and provide more accurate and relevant search results (including references). The following are the specific objectives of this project.

   * LLMs should be run locally. (The maximum usage of VRAM should be less than 80 GB) 
   * Read/handle various file formats (ppt, excel, word, pdf, and text).
   * Need to extract the exact contents or units of a table contained in the document.
   * Developed model should be able to handle English (or both Korean and English) doucumnets.
   * Need to return a reference list of its contents after searching.

<br/>

ğŸ“– ì°¸ê³  ë‚´ìš© 

   * í•´ë‹¹ ê³¼ì œ í‰ê°€ëŠ” ë…¼ë¬¸ í˜•ì‹ì˜ pdf, word, ê·¸ë¦¬ê³  ì£¼ì–´ì§„ ì›¹í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì§€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤. 
   * (Download Llama 2 model) [https://ai.meta.com/llama/] - 13B or 70B  model is preferred 
   * (Open platform for LLM based chatbot) [https://github.com/lm-sys/FastChat]
   * (Interface between LLMs and data) [https://github.com/run-llama/llama_index]


ì•„ë˜ëŠ” ëŒ€íšŒí˜• ì¸í„°í˜ì´ìŠ¤ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, target data í´ë”ì—ì„œ í•´ë‹¹ ë‚´ìš© / ìœ„ì¹˜ë¥¼ ëŒ€í™”í˜•ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. 
![image](https://github.com/CORNING-AI-CHALLENGE/item1/assets/146830948/8f1da305-38a7-45ec-9dd4-a3f7b560e43e)

